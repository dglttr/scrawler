<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Built-in Data Extractors" href="built_in_data_extractors.html" /><link rel="prev" title="Welcome to scrawler’s documentation!" href="index.html" />

    <meta name="generator" content="sphinx-3.5.4, furo 2021.04.11.beta34"/>
        <title>Getting Started - scrawler 0.3.0 documentation</title>
      <link rel="stylesheet" href="_static/styles/furo.css?digest=59ab60ac09ea94ccfe6deddff6d715cce948a6fc">
    <link rel="stylesheet" href="_static/pygments.css">
    <link media="(prefers-color-scheme: dark)" rel="stylesheet" href="_static/pygments_dark.css">
    


<style>
  :root {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media (prefers-color-scheme: dark) {
    :root {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
  }

  /* For allowing end-user-specific overrides */
  .override-light {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  .override-dark {
    --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
  }
</style><link rel="stylesheet" href="_static/styles/furo-extensions.css?digest=d391b54134226e4196576da3bdb6dddb7e05ba2b"></head>
  <body dir="">
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke-width="1.5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z"/>
      <line x1="4" y1="6" x2="20" y2="6" />
      <line x1="10" y1="12" x2="20" y2="12" />
      <line x1="6" y1="18" x2="20" y2="18" />
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
      stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
      class="feather feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">scrawler 0.3.0 documentation</div></a>
    </div>
    <div class="header-right">
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  
  <span class="sidebar-brand-text">scrawler 0.3.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html">
  <input class="sidebar-search" placeholder=Search name="q">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="built_in_data_extractors.html">Built-in Data Extractors</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_data_extractors.html">Custom Data Extractors</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Reference</a></li>
</ul>

</div>
</div>
      </div>
      
    </div>
  </aside>
  <main class="main">
    <div class="content">
      <article role="main">
        <label class="toc-overlay-icon toc-content-icon" for="__toc">
          <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
        </label>
        <div class="section" id="getting-started">
<h1>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h1>
<p>To get started, have a look at the <a class="reference external" href="https://github.com/dglttr/scrawler/tree/main/templates">templates</a> folder. It
contains four files, each one doing a different task. All templates include three sections:</p>
<ol class="arabic simple">
<li><p><strong>Imports</strong> retrieves all code dependencies from various files.</p></li>
<li><p><strong>Setup</strong> is where all parameters are specified.</p></li>
<li><p>In <strong>Execution</strong>, an instance of the respective Python object is created and its <code class="docutils literal notranslate"><span class="pre">run()</span></code> method executed.</p></li>
</ol>
<p>As a starting point, you can copy-and-paste a template and make any adjustments you would like.</p>
<p>Let’s have a closer look at the <strong>Setup</strong> section.</p>
<p>First, the <em>URL(s)</em> to be processed are specified.</p>
<p>Then, the <em>attributes</em> that define how to accomplish the tasks are specified:</p>
<div class="table-wrapper"><table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="reference.html#scrawler.attributes.SearchAttributes" title="scrawler.attributes.SearchAttributes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SearchAttributes</span></code></a></p></td>
<td><p>Specify which data to collect/search for in the website.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="reference.html#scrawler.attributes.ExportAttributes" title="scrawler.attributes.ExportAttributes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExportAttributes</span></code></a></p></td>
<td><p>Specify how and where to export the collected data.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="reference.html#scrawler.attributes.CrawlingAttributes" title="scrawler.attributes.CrawlingAttributes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CrawlingAttributes</span></code></a></p></td>
<td><p>Specify how to conduct the crawling, including filtering irrelevant URLs or limiting the number of crawled URLs.</p></td>
</tr>
</tbody>
</table></div>
<p>For more details, see the section <a class="reference internal" href="#attributes">Attributes</a>.</p>
<p>In the section <strong>Execution</strong>, these parameters are then passed to the relevant object (see next section).</p>
<div class="section" id="basic-objects">
<h2>Basic Objects<a class="headerlink" href="#basic-objects" title="Permalink to this headline">¶</a></h2>
<p>The basic functionality of <strong>scrawler</strong> is contained in two classes, <a class="reference internal" href="reference.html#scrawler.scraping.Scraper" title="scrawler.scraping.Scraper"><code class="xref py py-class docutils literal notranslate"><span class="pre">Scraper</span></code></a> and <a class="reference internal" href="reference.html#scrawler.crawling.Crawler" title="scrawler.crawling.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>.</p>
<div class="section" id="functionality">
<h3>Functionality<a class="headerlink" href="#functionality" title="Permalink to this headline">¶</a></h3>
<p>The objects are passed all relevant parameters during object initialization.
Then, three methods can be applied to them:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">run()</span></code>: Execute the task and return the results.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_and_export()</span></code>: This may be used when scraping/crawling many
sites at once, generating huge amounts of data. In order to prevent a
<a class="reference external" href="https://docs.python.org/3/library/exceptions.html#MemoryError" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">MemoryError</span></code></a>, data will be exported as soon as it is ready and
then discarded to make room for the next sites/domains.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export_data()</span></code>: Export the collected data to CSV file(s).</p></li>
</ul>
</div>
<div class="section" id="example-crawling">
<h3>Example Crawling<a class="headerlink" href="#example-crawling" title="Permalink to this headline">¶</a></h3>
<p>Let’s have a look at an example for <strong>crawling</strong> <code class="docutils literal notranslate"><span class="pre">https://example.com</span></code>.
For the moment, you can ignore the variables <code class="docutils literal notranslate"><span class="pre">search_attrs</span></code>, <code class="docutils literal notranslate"><span class="pre">export_attrs</span></code> and <code class="docutils literal notranslate"><span class="pre">crawling_attrs</span></code>.
We will get to them <a class="reference external" href="#attributes">later</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrawler</span> <span class="kn">import</span> <span class="n">Crawler</span>

<span class="n">search_attrs</span><span class="p">,</span> <span class="n">export_attrs</span><span class="p">,</span> <span class="n">crawling_attrs</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="o">...</span>

<span class="n">crawler</span> <span class="o">=</span> <span class="n">Crawler</span><span class="p">(</span><span class="s2">"https://example.com"</span><span class="p">,</span>
                  <span class="n">search_attributes</span><span class="o">=</span><span class="n">search_attrs</span><span class="p">,</span>
                  <span class="n">export_attributes</span><span class="o">=</span><span class="n">export_attrs</span><span class="p">,</span>
                  <span class="n">crawling_attributes</span><span class="o">=</span><span class="n">crawling_attrs</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">crawler</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="n">crawler</span><span class="o">.</span><span class="n">export_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="example-scraping">
<h3>Example Scraping<a class="headerlink" href="#example-scraping" title="Permalink to this headline">¶</a></h3>
<p>Here, multiple sites are <strong>scraped</strong> at once.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrawler</span> <span class="kn">import</span> <span class="n">Scraper</span>

<span class="n">search_attrs</span><span class="p">,</span> <span class="n">export_attrs</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span> <span class="o">...</span>

<span class="n">scraper</span> <span class="o">=</span> <span class="n">Scraper</span><span class="p">([</span><span class="s2">"https://www.example1.com"</span><span class="p">,</span> <span class="s2">"https://www.example2.com"</span><span class="p">,</span> <span class="s2">"https://www.example3.com"</span><span class="p">],</span>
                  <span class="n">search_attributes</span><span class="o">=</span><span class="n">search_attrs</span><span class="p">,</span>
                  <span class="n">export_attributes</span><span class="o">=</span><span class="n">export_attrs</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">scraper</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="n">scraper</span><span class="o">.</span><span class="n">export_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="attributes">
<h2>Attributes<a class="headerlink" href="#attributes" title="Permalink to this headline">¶</a></h2>
<p>Now that we know the objects that will perform our tasks, we would like to specify exactly how to go about it.</p>
<div class="section" id="search-attributes">
<h3>Search Attributes<a class="headerlink" href="#search-attributes" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference internal" href="reference.html#scrawler.attributes.SearchAttributes" title="scrawler.attributes.SearchAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">SearchAttributes</span></code></a> specify which data to collect/search for in
the website (and how to do it). This is done by passing data extractor
objects to <a class="reference internal" href="reference.html#scrawler.attributes.SearchAttributes" title="scrawler.attributes.SearchAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">SearchAttributes</span></code></a> during initialization.</p>
<p>There are many data extractors already build into the project, see <a class="reference external" href="built_in_data_extractors.html">built-in data extractors</a>.
You can also specify your own <a class="reference external" href="custom_data_extractors.html">custom data extractors</a>.</p>
<p>In this example, we set up <a class="reference internal" href="reference.html#scrawler.attributes.SearchAttributes" title="scrawler.attributes.SearchAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">SearchAttributes</span></code></a> that will extract three different data points from websites,
specified using the built-in <a class="reference internal" href="reference.html#scrawler.data_extractors.UrlExtractor" title="scrawler.data_extractors.UrlExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">UrlExtractor</span></code></a>, <a class="reference internal" href="reference.html#scrawler.data_extractors.TitleExtractor" title="scrawler.data_extractors.TitleExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TitleExtractor</span></code></a> and <a class="reference internal" href="reference.html#scrawler.data_extractors.DateExtractor" title="scrawler.data_extractors.DateExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DateExtractor</span></code></a> data extractors.
Note how parameters for the data extractors are passed directly during initialization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrawler.attributes</span> <span class="kn">import</span> <span class="n">SearchAttributes</span>
<span class="kn">from</span> <span class="nn">scrawler.data_extractors</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">search_attrs</span> <span class="o">=</span> <span class="n">SearchAttributes</span><span class="p">(</span>
    <span class="n">UrlExtractor</span><span class="p">(),</span>  <span class="c1"># returns URL</span>
    <span class="n">TitleExtractor</span><span class="p">(),</span>  <span class="c1"># returns website &lt;title&gt; tag content</span>
    <span class="n">DateExtractor</span><span class="p">(</span><span class="n">tag_types</span><span class="o">=</span><span class="s2">"meta"</span><span class="p">,</span> <span class="n">tag_attrs</span><span class="o">=</span><span class="p">{</span><span class="s2">"name"</span><span class="p">:</span> <span class="s2">"pubdate"</span><span class="p">})</span>  <span class="c1"># returns publication date from pubdate meta tag</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="reference.html#scrawler.attributes.SearchAttributes" title="scrawler.attributes.SearchAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">SearchAttributes</span></code></a>: More detailed documentation.</p>
</div>
</div>
<div class="section" id="export-attributes">
<h3>Export Attributes<a class="headerlink" href="#export-attributes" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference internal" href="reference.html#scrawler.attributes.ExportAttributes" title="scrawler.attributes.ExportAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExportAttributes</span></code></a> specify how and where to export the collected
data to. Data is always exported to the CSV format, therefore the
various parameters are geared towards the CSV format.</p>
<p>Two parameters <em>must</em> be specified here:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">directory</span></code>: The directory (folder) that the file(s) will be saved to.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fn</span></code>: Filename(s) of the exported CSV files containing the crawled data.
You don’t have to specify the file extension <code class="docutils literal notranslate"><span class="pre">.csv</span></code>, since the files will always be CSV files
(for example, use <code class="docutils literal notranslate"><span class="pre">crawled_data</span></code> instead of <code class="docutils literal notranslate"><span class="pre">crawled_data.csv</span></code>).</p></li>
</ul>
<p>Here’s an exemplary <a class="reference internal" href="reference.html#scrawler.attributes.ExportAttributes" title="scrawler.attributes.ExportAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExportAttributes</span></code></a> object creation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrawler.attributes</span> <span class="kn">import</span> <span class="n">ExportAttributes</span>

<span class="n">export_attrs</span> <span class="o">=</span> <span class="n">ExportAttributes</span><span class="p">(</span>
    <span class="n">directory</span><span class="o">=</span><span class="sa">r</span><span class="s2">"C:\Users\USER\Documents"</span><span class="p">,</span>
    <span class="n">fn</span><span class="o">=</span><span class="p">[</span><span class="s2">"example1_crawled_data"</span><span class="p">,</span> <span class="s2">"example1_crawled_data"</span><span class="p">,</span> <span class="s2">"example1_crawled_data"</span><span class="p">],</span>
    <span class="n">header</span><span class="o">=</span><span class="p">[</span><span class="s2">"URL"</span><span class="p">,</span> <span class="s2">"Title"</span><span class="p">,</span> <span class="s2">"Publication Date"</span><span class="p">],</span>
    <span class="n">separator</span><span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="reference.html#scrawler.attributes.ExportAttributes" title="scrawler.attributes.ExportAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExportAttributes</span></code></a>: More detailed documentation.</p>
</div>
</div>
<div class="section" id="crawling-attributes">
<h3>Crawling Attributes<a class="headerlink" href="#crawling-attributes" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference internal" href="reference.html#scrawler.attributes.CrawlingAttributes" title="scrawler.attributes.CrawlingAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlingAttributes</span></code></a> specify how to conduct the crawling, e.g.
how to filter irrelevant URLs or limits on the number of URLs crawled.
As implied by their name, they are only relevant for crawling tasks.</p>
<p>Some commonly adjusted parameters include:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">filter_foreign_urls</span></code>: This parameter defines how the crawler knows
that a given URL is still part of the target domain. For example, one
may only want to crawl a subdomain, not the entire domain (only URLs
from <code class="docutils literal notranslate"><span class="pre">subdomain.example.com</span></code> vs. the entire <code class="docutils literal notranslate"><span class="pre">example.com</span></code>
domain). Details on valid input values can be found in the
documentation for <a class="reference internal" href="reference.html#scrawler.attributes.CrawlingAttributes" title="scrawler.attributes.CrawlingAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlingAttributes</span></code></a>. By default,
this is set to <code class="docutils literal notranslate"><span class="pre">auto</span></code>, which means that the correct mode will be
inferred by looking at the passed base/start URL. For example, if the
start URL contains a subdomain, only links from the subdomain will be
crawled. For details, refer to the documentation for the
<a class="reference internal" href="reference.html#scrawler.utils.web_utils.extract_same_host_pattern" title="scrawler.utils.web_utils.extract_same_host_pattern"><code class="xref py py-func docutils literal notranslate"><span class="pre">extract_same_host_pattern()</span></code></a> function. Note that you can also pass
your own comparison function here. It has to include two parameters,
<code class="docutils literal notranslate"><span class="pre">url1</span></code> and <code class="docutils literal notranslate"><span class="pre">url2</span></code>. The first URL is the one to be checked, and
the second is the reference (the crawling start URL). This function
should return <code class="docutils literal notranslate"><span class="pre">True</span></code> for URLs that belong to the same host, and
<code class="docutils literal notranslate"><span class="pre">False</span></code> for foreign URLs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">filter_media_files</span></code>: Controls whether to filter out (ignore) media
files. Media files can be quite large and make the crawling process
significantly longer, while not adding any new information because
media file data can’t be parsed and processed. Therefore, the crawler
filters media by looking at the URL (e.g. URLs ending in <code class="docutils literal notranslate"><span class="pre">.pdf</span></code> or
<code class="docutils literal notranslate"><span class="pre">.jpg</span></code>), as well as the response header
<a class="reference external" href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type">content-type</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">blocklist</span></code>: Some directories might not be interesting for the
crawling process (e.g., <code class="docutils literal notranslate"><span class="pre">/media/</span></code>). The <code class="docutils literal notranslate"><span class="pre">blocklist</span></code> parameter
makes it possible to pass a list of strings that might occur in a
URL. If the URL contains any of the given strings, it is filtered
out.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_no_urls</span></code>: Some domains contain many webpages. This parameter
can be passed an integer as the maximum total amount of URLs to be
crawled.</p></li>
</ul>
<p>Here’s an exemplary <a class="reference internal" href="reference.html#scrawler.attributes.CrawlingAttributes" title="scrawler.attributes.CrawlingAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlingAttributes</span></code></a> object creation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrawler.attributes</span> <span class="kn">import</span> <span class="n">CrawlingAttributes</span>

<span class="n">DOMAIN_TO_CRAWL</span> <span class="o">=</span> <span class="s2">"https://www.blog.example.com"</span>

<span class="n">crawling_attrs</span> <span class="o">=</span> <span class="n">CrawlingAttributes</span><span class="p">(</span>
    <span class="n">filter_foreign_urls</span><span class="o">=</span><span class="s2">"subdomain1"</span><span class="p">,</span>  <span class="c1"># only crawling the `blog` subdomain</span>
    <span class="n">filter_media_files</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">blocklist</span><span class="o">=</span><span class="p">(</span><span class="s2">"git."</span><span class="p">,</span> <span class="s2">"datasets."</span><span class="p">,</span> <span class="s2">"nextcloud."</span><span class="p">),</span>
    <span class="n">max_no_urls</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Another example with a <em>custom foreign URL filter</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tld.exceptions</span>

<span class="kn">from</span> <span class="nn">scrawler.attributes</span> <span class="kn">import</span> <span class="n">CrawlingAttributes</span>
<span class="kn">from</span> <span class="nn">scrawler.utils.web_utils</span> <span class="kn">import</span> <span class="n">ParsedUrl</span>

<span class="n">DOMAIN_TO_CRAWL</span> <span class="o">=</span> <span class="s2">"https://www.blog.example.com/my_directory/index.html"</span>


<span class="k">def</span> <span class="nf">should_be_crawled</span><span class="p">(</span><span class="n">url1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">url2</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">"""Custom foreign URL filter: Crawl all URLs from host `www.blog.example.com` inside the directory `my_directory`."""</span>
    <span class="k">try</span><span class="p">:</span>  <span class="c1"># don't forget exception handling</span>
        <span class="n">url1</span> <span class="o">=</span> <span class="n">ParsedUrl</span><span class="p">(</span><span class="n">url1</span><span class="p">)</span>
        <span class="n">url2</span> <span class="o">=</span> <span class="n">ParsedUrl</span><span class="p">(</span><span class="n">url2</span><span class="p">)</span>
    <span class="k">except</span> <span class="p">(</span><span class="n">tld</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">TldBadUrl</span><span class="p">,</span> <span class="n">tld</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">TldDomainNotFound</span><span class="p">):</span>  <span class="c1"># URL couldn't be parsed</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">return</span> <span class="p">((</span><span class="n">url1</span><span class="o">.</span><span class="n">hostname</span> <span class="o">==</span> <span class="n">url2</span><span class="o">.</span><span class="n">hostname</span><span class="p">)</span>  <span class="c1"># hostname is `www.blog.example.com`</span>
            <span class="ow">and</span> <span class="p">(</span><span class="s2">"my_directory"</span> <span class="ow">in</span> <span class="n">url1</span><span class="o">.</span><span class="n">path</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="s2">"my_directory"</span> <span class="ow">in</span> <span class="n">url2</span><span class="o">.</span><span class="n">path</span><span class="p">))</span>


<span class="n">crawling_attrs</span> <span class="o">=</span> <span class="n">CrawlingAttributes</span><span class="p">(</span>
    <span class="n">filter_foreign_urls</span><span class="o">=</span><span class="n">should_be_crawled</span><span class="p">,</span>  <span class="c1"># pass custom foreign URL filter here</span>
    <span class="n">filter_media_files</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">blocklist</span><span class="o">=</span><span class="p">(</span><span class="s2">"git."</span><span class="p">,</span> <span class="s2">"datasets."</span><span class="p">,</span> <span class="s2">"nextcloud."</span><span class="p">),</span>
    <span class="n">max_no_urls</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="reference.html#scrawler.attributes.CrawlingAttributes" title="scrawler.attributes.CrawlingAttributes"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlingAttributes</span></code></a>: More detailed documentation.</p>
</div>
</div>
<div class="section" id="other-settings">
<h3>Other Settings<a class="headerlink" href="#other-settings" title="Permalink to this headline">¶</a></h3>
<p>Most parameters are encompassed in the three attribute objects above.
However, there are some additional settings available for special cases.</p>
<p>If you look at the templates’ <strong>Setup</strong> section again, it includes a <code class="docutils literal notranslate"><span class="pre">USER_AGENT</span></code> parameter that sets the
<a class="reference external" href="https://en.wikipedia.org/wiki/User_agent">user agent</a> to be used during scraping/crawling.</p>
<p>Finally, <a class="reference external" href="https://github.com/dglttr/scrawler/blob/main/scrawler/defaults.py">defaults.py</a>
contains standard settings that are used throughout the project.</p>
</div>
</div>
<div class="section" id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline">¶</a></h2>
<div class="section" id="why-are-there-two-backends">
<h3>Why are there two backends?<a class="headerlink" href="#why-are-there-two-backends" title="Permalink to this headline">¶</a></h3>
<p>The module <a class="reference external" href="https://github.com/dglttr/scrawler/tree/main/scrawler/backends">backends</a> contains two files with
the same functions for scraping and crawling, but built on different
technologies for parallelization: One uses <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.9)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> and the other <a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing" title="(in Python v3.9)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">multiprocessing</span></code></a>,
more precisely using <em>multithreading</em> by means of <a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.dummy" title="(in Python v3.9)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">multiprocessing.dummy</span></code></a>.</p>
<p>In general, <a class="reference internal" href="reference.html#module-scrawler.backends.asyncio_backend" title="scrawler.backends.asyncio_backend"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio_backend</span></code></a> is preferable because more sites can be processed in parallel.
However, on very large sites, scrawler may get stuck, and the entire crawling will hang.
Also, <a class="reference external" href="https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ServerDisconnectedError" title="(in aiohttp v3.7)"><code class="docutils literal notranslate"><span class="pre">aiohttp.ServerDisconnectedError</span></code></a> may occur a lot.
If you expect or experience these cases, it is preferable to use the
<a class="reference internal" href="reference.html#module-scrawler.backends.multithreading_backend" title="scrawler.backends.multithreading_backend"><code class="xref py py-class docutils literal notranslate"><span class="pre">multithreading_backend</span></code></a>, which is slower, but more robust.</p>
</div>
</div>
</div>

      </article>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="built_in_data_extractors.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Built-in Data Extractors</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="index.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Home</div>
                
              </div>
            </a>
        </div>

        <div class="related-information">
              Copyright &#169; 2021, Daniel Glatter
            |
            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a>
              and
              <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
              <a href="https://github.com/pradyunsg/furo">Furo theme</a>.
            |
            <a class="muted-link" href="_sources/getting_started.rst.txt"
               rel="nofollow">
              Show Source
            </a>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Getting Started</a><ul>
<li><a class="reference internal" href="#basic-objects">Basic Objects</a><ul>
<li><a class="reference internal" href="#functionality">Functionality</a></li>
<li><a class="reference internal" href="#example-crawling">Example Crawling</a></li>
<li><a class="reference internal" href="#example-scraping">Example Scraping</a></li>
</ul>
</li>
<li><a class="reference internal" href="#attributes">Attributes</a><ul>
<li><a class="reference internal" href="#search-attributes">Search Attributes</a></li>
<li><a class="reference internal" href="#export-attributes">Export Attributes</a></li>
<li><a class="reference internal" href="#crawling-attributes">Crawling Attributes</a></li>
<li><a class="reference internal" href="#other-settings">Other Settings</a></li>
</ul>
</li>
<li><a class="reference internal" href="#faq">FAQ</a><ul>
<li><a class="reference internal" href="#why-are-there-two-backends">Why are there two backends?</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </main>
</div>
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/scripts/main.js?digest=e931d09b2a40c1bb82b542effe772014573baf67"></script></body>
</html>