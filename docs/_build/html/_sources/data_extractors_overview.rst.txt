Data Extractors
===============

Data extractors are functions used to retrieve various data points from `Website <website.html>`__ objects.

Which extractors are available?
-------------------------------

There are many data extractors already build into the project, see `built-in data extractors <built_in_data_extractors.html>`__.
You can also specify your own `custom data extractors <custom_data_extractors.html>`__.

``BaseExtractor``
-----------------

All extractors inherit from ``BaseExtractor`` (documented `here <reference.html#scrawler.data_extractors.BaseExtractor>`__).

The following provides some additional notes on the basic parameters specified in ``BaseExtractor`` that apply to all data extractors.

``n_return_values``
~~~~~~~~~~~~~~~~~~~
The parameter ``n_return_values`` specifies the number of values
that will be returned by the extractor. This is almost always 1, but
there are cases such as ``DateExtractor`` which may return more values.
If you build your own data extractor that may return more than one
value, make sure to update ``self.n_return_values``. This attribute is
used to validate that the length of the header of the CSV file equals
the number of columns generated by the search attributes. Have a look at
the implementation of ``DateExtractor`` to see how this might be handled.

``dynamic_parameters``
~~~~~~~~~~~~~~~~~~~~~~
The parameter ``dynamic_parameters`` handles a special case of data
extraction when scraping/crawling multiple sites. There may be cases
where you would like to have a different set of parameters for each URL.
In this case, you can pass the relevant parameter as a list and set
``dynamic_parameters`` to True. The scraper/crawler will then have each
URL/scraping target use a different value from that list based on an
index. In this example, a different ID will be put for each crawled domain:

.. code:: python

   from scrawler.data_extractors import CustomStringPutter

   DOMAINS_TO_CRAWL = ["https://www.abc.com", "https://www.def.com", "https://www.ghi.com"]
   putter = CustomStringPutter(["id_1001", "id_1002", "id_1003"], use_index=True)

Note that when enabling ``dynamic_parameters``, to parameters going into
this data extractor can only have one of two forms:

-  A list (not a tuple!) where each list entry matches *exactly one* URL
   (in the same order as in the list of the URLs, see variable
   ``DOMAINS_TO_CRAWL`` in the example above).
-  A constant (of a type other than list) than will be the same for all
   URLs.

Passing a parameter list shorter or longer than the list of URLs will
raise an error.

All built-in data extractors support dynamic parameters and you can
easily add support to your custom data extractor by using the
``supports_dynamic_parameters`` function decorator to decorate the
``run()`` method, like this:

.. code:: python

   from scrawler import Website
   from scrawler.data_extractors import BaseExtractor, supports_dynamic_parameters


   class CopyrightExtractor(BaseExtractor):
       def __init__(self, **kwargs):
           """Extract website copyright tag."""
           super().__init__(**kwargs)

       @supports_dynamic_parameters
       def run(self, website: Website, index: int = None):
           copyright_tag = website.find("meta", attrs={"name": "copyright"})

           # Important: Do not forget to handle exceptions, because many sites will not have this copyright tag
           try:
               copyright_text = copyright_tag.attrs["content"]
           except (AttributeError, KeyError):
               copyright_text = "NULL"

           return copyright_text
